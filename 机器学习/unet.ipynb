{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "unet.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unXNa980dcGg",
    "colab_type": "text"
   },
   "source": [
    "unet是一个图像分割网络，论文是U-Net: Convolutional Networks for Biomedical Image Segmentation。它在ISBI 2015的细胞跟踪挑战中取得了最好的成绩，因此很有研究的价值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkK-PvZMNE3I",
    "colab_type": "text"
   },
   "source": [
    "# 挂载网盘和准备数据\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "u0YQ0cks0ej1",
    "colab_type": "code",
    "outputId": "39ccd3b7-781d-40b9-9c12-cec30e7b89aa",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    }
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EFv1tG5QvF1H",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "!cp -r /content/drive/My\\ Drive/unet-data ./"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdiXxjXceNrc",
    "colab_type": "text"
   },
   "source": [
    "# 数据扩展\n",
    "ISBI挑战是关于医学图像方面的，因此能够取得的训练数据集有限，这个时候，适当的进行数据扩展操作，可让我们的训练结果更可靠。keras中提供了现有的图像扩展函数:\n",
    "keras.preprocessing.image.ImageDataGenerator(),它的参数有很多，可以查看[keras documentation ](https://keras.io/preprocessing/image/),这里还有篇中文介绍[Keras花式工具箱](https://zhuanlan.zhihu.com/p/30197320)。下面我们用这个工具来扩展现有的医学图像,需要注意的是原始图像和它对应的分割图像应该具有相同的变换参数，这个可以通过相同的seed参数来实现"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KaFmcqNIjDj-",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "def train_augmentation(batch_size,train_path,image_folder,mask_folder,aug_dict,image_color_mode = \"grayscale\",\n",
    "                    mask_color_mode = \"grayscale\",image_save_prefix  = \"image\",mask_save_prefix  = \"mask\",\n",
    "                    flag_multi_class = False,num_class = 2,save_to_dir = None,target_size = (512,512),seed = 11):\n",
    "  image_datagen = ImageDataGenerator(**aug_dict)\n",
    "  mask_datagen = ImageDataGenerator(**aug_dict)\n",
    "  image_generator = image_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes = [image_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = image_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = image_save_prefix,\n",
    "        seed = seed)\n",
    "  mask_generator = mask_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes = [mask_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = mask_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = mask_save_prefix,\n",
    "        seed = seed)\n",
    "  train_generator = zip(image_generator, mask_generator)\n",
    "  for (img,mask) in train_generator:\n",
    "    yield (img,mask)\n",
    "#利用上面的函数我们产生10倍的扩展数据\n",
    "data_gen_args = dict(rotation_range=1,\n",
    "                    width_shift_range=0.05,\n",
    "                    height_shift_range=0.05,\n",
    "                    shear_range=0.05,\n",
    "                    zoom_range=0.05,\n",
    "                    horizontal_flip=True,\n",
    "                    fill_mode='nearest')\n",
    "myGenerator = train_augmentation(20,'unet-data/train','image','label',data_gen_args,save_to_dir = \"unet-data/train/aug\")\n",
    "num_batch = 10\n",
    "for i,batch in enumerate(myGenerator):\n",
    "    if(i >= num_batch):\n",
    "        break"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-XoFSLe3jyK",
    "colab_type": "text"
   },
   "source": [
    "有了训练用数据，我们接下来看看unet网络的具体结构，这里直接放论文中的网络结构图：\n",
    "![unet网络](https://i.ibb.co/P4Qhgms/Snipaste-2019-09-27-19-33-13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90GTSzdngxe3",
    "colab_type": "text"
   },
   "source": [
    "这里的所有卷积操作都不需要填充，然后up-conv操作是先一个对feature map上采样，然后接一个2*2的卷积网络，在卷积网络中降低一半的通道数。这里我们先用keras实现这个网络"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xsF_WxCPvq75",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "import sys\n",
    "\n",
    "def custom_loss(weights):\n",
    "  def w_loss(y_true, y_pred):\n",
    "    batch_loss=tf.where(y_true>0,tf.log(y_pred),y_true)\n",
    "    weighted_loss=-tf.multiply(weights,batch_loss)\n",
    "    return K.mean(K.sum(weighted_loss,axis=[1,2,3]))\n",
    "  return w_loss\n",
    "\n",
    "def unet(pretrained_weights = None,input_size = (572,572,1),weight_size=(388,388,2)):\n",
    "  inputs = Input(input_size)\n",
    "  input_weights=Input(weight_size)\n",
    "  conv1=Conv2D(64,3,activation='relu',kernel_initializer='he_normal')(inputs)\n",
    "  conv1=Conv2D(64,3,activation='relu',kernel_initializer='he_normal')(conv1)\n",
    "  pool1=MaxPooling2D(pool_size=(2,2))(conv1)\n",
    "  conv2=Conv2D(128,3,activation='relu',kernel_initializer='he_normal')(pool1)\n",
    "  conv2=Conv2D(128,3,activation='relu',kernel_initializer='he_normal')(conv2)\n",
    "  pool2=MaxPooling2D(pool_size=(2,2))(conv2)\n",
    "  conv3=Conv2D(256,3,activation='relu',kernel_initializer='he_normal')(pool2)\n",
    "  conv3=Conv2D(256,3,activation='relu',kernel_initializer='he_normal')(conv3)\n",
    "  pool3=MaxPooling2D(pool_size=(2,2))(conv3)\n",
    "  conv4=Conv2D(512,3,activation='relu',kernel_initializer='he_normal')(pool3)\n",
    "  conv4=Conv2D(512,3,activation='relu',kernel_initializer='he_normal')(conv4)\n",
    "  pool4=MaxPooling2D(pool_size=(2,2))(conv4)\n",
    "  conv5=Conv2D(1024,3,activation='relu',kernel_initializer='he_normal')(pool4)\n",
    "  conv5=Conv2D(1024,3,activation='relu',kernel_initializer='he_normal')(conv5)\n",
    "  up6=Conv2D(512,2,activation='relu',padding = 'same',kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv5))\n",
    "  crop4=Cropping2D((4,4))(conv4)\n",
    "  merge6=concatenate([crop4,up6],axis=3)\n",
    "  conv6=Conv2D(512,3,activation='relu',kernel_initializer='he_normal')(merge6)\n",
    "  conv6=Conv2D(512,3,activation='relu',kernel_initializer='he_normal')(conv6)\n",
    "  up7=Conv2D(256,2,activation='relu',padding = 'same',kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv6))\n",
    "  crop3=Cropping2D((16,16))(conv3)\n",
    "  merge7=concatenate([crop3,up7],axis=3)\n",
    "  conv7=Conv2D(256,3,activation='relu',kernel_initializer='he_normal')(merge7)\n",
    "  conv7=Conv2D(256,3,activation='relu',kernel_initializer='he_normal')(conv7)\n",
    "  up8=Conv2D(512,2,activation='relu',padding = 'same',kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv7))\n",
    "  crop2=Cropping2D((40,40))(conv2)\n",
    "  merge8=concatenate([crop2,up8],axis=3)\n",
    "  conv8=Conv2D(128,3,activation='relu',kernel_initializer='he_normal')(merge8)\n",
    "  conv8=Conv2D(128,3,activation='relu',kernel_initializer='he_normal')(conv8)\n",
    "  up9=Conv2D(64,2,activation='relu',padding = 'same',kernel_initializer='he_normal')(UpSampling2D(size=(2, 2))(conv8))\n",
    "  crop1=Cropping2D((88,88))(conv1)\n",
    "  merge9=concatenate([crop1,up9],axis=3)\n",
    "  conv9=Conv2D(64,3,activation='relu',kernel_initializer='he_normal')(merge9)\n",
    "  conv9=Conv2D(64,3,activation='relu',kernel_initializer='he_normal')(conv9)\n",
    "  conv10 = Conv2D(2, 1, activation = 'relu',kernel_initializer='he_normal')(conv9)\n",
    "  out_softmax=Softmax()(conv10)\n",
    "  model = Model(inputs = [inputs,input_weights], outputs = out_softmax)\n",
    "  wloss = custom_loss(weights=input_weights)\n",
    "  model.compile(optimizer = Adam(lr = 1e-4), loss = wloss, metrics = ['categorical_accuracy'])\n",
    "  #model.summary()\n",
    "  if(pretrained_weights):\n",
    "    model.load_weights(pretrained_weights)\n",
    "  return model"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nUsing TensorFlow backend.\n"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yji75YjVJeaA",
    "colab_type": "text"
   },
   "source": [
    "上面定义的unet网络结构中，我们不仅需要训练用输入图片，而且还需要权重weight输入，那么这个权重是怎么计算的呢？首先，细胞和细胞壁出现概率是不一样的，那么我们需要为这两类像素点，分配不同的权重，以平衡这个概率。,然后论文中为了加强对邻近细胞边界的识别，对这部分边界数据的权重进行了增加，使用的公式是:\n",
    "$$\n",
    "w(x)=w_c(x)+w_0\\cdot exp(-\\frac{(d_1(x)+d_2(x))^2}{2\\sigma^2})\n",
    "$$\n",
    "这里的$w_c$是上面我们计算出的相应类别的权重，然后$d_1$表示$x$到最近细胞的距离，$d_2$表示$x$到第二近的细胞的距离，然后$w_0=10,\\sigma \\approx 5$。下面对每一个mask图像，我们来计算对应的权重:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KAxU1Nu53-k8",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from skimage import io\n",
    "from skimage import measure\n",
    "from scipy.ndimage.morphology import distance_transform_edt\n",
    "\n",
    "\n",
    "def unet_wmap(mask_img,w0=10.0,sigma=5):\n",
    "  denominator=2*sigma*sigma\n",
    "  cell_wall_num=(mask_img<255/2).sum()\n",
    "  cell_num=mask_img.size-cell_wall_num\n",
    "  weights=np.asarray([1.0/cell_num,1.0/cell_wall_num])\n",
    "  max_w=np.max(weights)\n",
    "  #为了使最少的class权重为1\n",
    "  weights=weights/max_w\n",
    "  weight_map=np.empty_like(mask_img,dtype=np.float32)\n",
    "  weight_map[np.where(mask_img>=255/2)]=weights[0]\n",
    "  weight_map[np.where(mask_img<255/2)]=weights[1]\n",
    "  #计算联通区域\n",
    "  cells_image,cells_count = measure.label(mask_img>=255/2,return_num=True)\n",
    "  maps=np.zeros((mask_img.shape[0],mask_img.shape[1],cells_count))\n",
    "  if cells_count >= 2:\n",
    "    for ci in range(cells_count):\n",
    "      maps[:,:,ci]=distance_transform_edt(cells_image!=(ci+1))\n",
    "    maps=np.sort(maps,-1)\n",
    "    d1=maps[:,:,0]\n",
    "    d2=maps[:,:,1]\n",
    "    weight_map=weight_map+w0*np.exp(-np.power(d1+d2,2)/denominator)\n",
    "  return weight_map\n",
    "\n",
    "train_image_dir=\"/content/unet-data/train/aug\"\n",
    "files=os.listdir(train_image_dir)\n",
    "train_images=[]\n",
    "for f in files:\n",
    "  if f.startswith('image'):\n",
    "    item=dict()\n",
    "    item['image']=f\n",
    "    item['mask']='mask'+f[5:]\n",
    "    train_images.append(item)\n",
    "\n",
    "for item in train_images:\n",
    "  mask_img=io.imread(os.path.join(train_image_dir,item['mask']))\n",
    "  weight_img=unet_wmap(mask_img)\n",
    "  weight_img_name='weight'+item['mask'][4:-3]+\"npy\"\n",
    "  np.save(os.path.join(train_image_dir,weight_img_name),weight_img)\n",
    "  item['weight']=weight_img_name"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meEYL-XiNlP_",
    "colab_type": "text"
   },
   "source": [
    "有了这些权重之后，我们还需要解决一个问题，就是unet网络中输入的是$572*572$的图像，输出的却只是$388*388$的图像。对于这种情况，怎么对应到原始的输入和输出呢，论文中介绍了一种Overlap-tile策略：\n",
    "![overlap-tile](https://i.ibb.co/K5ZTCPx/Snipaste-2019-09-30-14-08-52.png)\n",
    "\n",
    "\n",
    "*   上图是针对任意大小的输入图像的无缝分割的 Overlap-tile 策略。如果我们要预测黄色框内区域（即对黄色的内的细胞进行分割，获取它们的边缘），需要将蓝色框内部分作为输入，如果换色区域在输入图像的边缘的话，那么缺失的数据使用镜像进行补充。如上图左边图像所示，输入图像周围一圈都进行了镜像补充。\n",
    "\n",
    "*   因为进行的是valid卷积，即上下文只取有效部分，可以理解为padding为0，卷积之后的图像尺寸会改变，所以需要取比黄色框大的图像来保证上下文的信息是有意义的，缺失的部分用镜像的方法补充是填充上下文信息最好的方法了。这种方法通常需要将图像进行分块的时候才使用。\n",
    "\n",
    "所以，对于$512*512$的输出图片，我们可以从4个角切出4份数据来进行训练："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "51qNE3TQ2IfE",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "def train_generator(batch_size,train_dir):\n",
    "  files=os.listdir(train_dir)\n",
    "  origin_train_items=[]\n",
    "  for f in files:\n",
    "    if f.startswith('image'):\n",
    "      item=dict()\n",
    "      item['image']=os.path.join(train_dir,f)\n",
    "      item['mask']=os.path.join(train_dir,'mask'+f[5:])\n",
    "      item['weight']=os.path.join(train_dir,'weight'+f[5:-3]+\"npy\")\n",
    "      #每幅图像可以产生4个训练数据\n",
    "      for corner in ('lt','rt','lb','rb'):\n",
    "        c_item=copy.deepcopy(item)\n",
    "        c_item['corner']=corner\n",
    "        origin_train_items.append(c_item)\n",
    "\n",
    "  while True:\n",
    "    train_items=copy.deepcopy(origin_train_items)\n",
    "    random.shuffle(train_items)\n",
    "    while len(train_items) >0:\n",
    "      need_count=min(batch_size,4*len(train_items))\n",
    "      image_batch=np.empty((batch_size,572,572,1))\n",
    "      mask_batch=np.empty((batch_size,388,388,2))\n",
    "      weight_batch=np.empty((batch_size,388,388,2))\n",
    "      for i in range(need_count):\n",
    "        item=train_items.pop()\n",
    "        image=io.imread(item['image']).astype(float)/255\n",
    "        image=np.pad(image,((92,92),(92,92)),'symmetric')\n",
    "        cell_mask=io.imread(item['mask']).astype(float)/255\n",
    "        #二值化\n",
    "        cell_mask[cell_mask<=0.5]=0\n",
    "        cell_mask[cell_mask>0.5]=1\n",
    "        non_cell_mask=1-cell_mask  \n",
    "        weight=np.load(item['weight'])\n",
    "        if item['corner'] == 'lt':\n",
    "          image_batch[i,:,:,0]=image[0:572,0:572]\n",
    "          mask_batch[i,:,:,0]=cell_mask[0:388,0:388]\n",
    "          mask_batch[i,:,:,1]=non_cell_mask[0:388,0:388]\n",
    "          weight_batch[i,:,:,0]=weight[0:388,0:388]\n",
    "          weight_batch[i,:,:,1]=weight[0:388,0:388]\n",
    "        elif item['corner'] == 'rt':\n",
    "          image_batch[i,:,:,0]=image[-572:,0:572]\n",
    "          mask_batch[i,:,:,0]=cell_mask[-388:,0:388]\n",
    "          mask_batch[i,:,:,1]=non_cell_mask[-388:,0:388]\n",
    "          weight_batch[i,:,:,0]=weight[-388:,0:388]\n",
    "          weight_batch[i,:,:,1]=weight[-388:,0:388]\n",
    "        elif item['corner'] == 'lb':\n",
    "          image_batch[i,:,:,0]=image[0:572,-572:]\n",
    "          mask_batch[i,:,:,0]=cell_mask[0:388,-388:]\n",
    "          mask_batch[i,:,:,1]=non_cell_mask[0:388,-388:]\n",
    "          weight_batch[i,:,:,0]=weight[0:388,-388:]\n",
    "          weight_batch[i,:,:,1]=weight[0:388,-388:]\n",
    "        elif item['corner'] == 'rb':\n",
    "          image_batch[i,:,:,0]=image[-572:,-572:]\n",
    "          mask_batch[i,:,:,0]=cell_mask[-388:,-388:]\n",
    "          mask_batch[i,:,:,1]=non_cell_mask[-388:,-388:]\n",
    "          weight_batch[i,:,:,0]=weight[-388:,-388:]\n",
    "          weight_batch[i,:,:,1]=weight[-388:,-388:]\n",
    "      yield ([image_batch,weight_batch],mask_batch)"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTeBP8iDSqwK",
    "colab_type": "text"
   },
   "source": [
    "现在有了数据和神经网络的定义，接下来就可以开始训练了"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "c4FA0mHjS1pO",
    "colab_type": "code",
    "outputId": "a9c55f79-c2f1-4ed8-d0a1-41a6e67abf54",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 737
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "\n",
    "K.clear_session()\n",
    "model_checkpoint = ModelCheckpoint('./unet_membrane.hdf5', monitor='loss',verbose=1, save_best_only=True,save_weights_only=True)\n",
    "data_gen=train_generator(4,r'C:\\project\\data\\unet-data\\train\\aug')\n",
    "model = unet('./unet_membrane.hdf5')\n",
    "model.fit_generator(data_gen,steps_per_epoch=170,epochs=100,callbacks=[model_checkpoint])"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Epoch 1/100\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ecce1cc023f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdata_gen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34mr'C:\\project\\data\\unet-data\\train\\aug'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./unet_membrane.hdf5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m170\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    529\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[4,128,393,393] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node conv2d_20/convolution}} = Conv2D[T=DT_FLOAT, _class=[\"loc:@training/Adam/gradients/conv2d_20/convolution_grad/Conv2DBackpropInput\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training/Adam/gradients/conv2d_20/convolution_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, conv2d_20/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node loss/mul/_595}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2771_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ],
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[4,128,393,393] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node conv2d_20/convolution}} = Conv2D[T=DT_FLOAT, _class=[\"loc:@training/Adam/gradients/conv2d_20/convolution_grad/Conv2DBackpropInput\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training/Adam/gradients/conv2d_20/convolution_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, conv2d_20/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node loss/mul/_595}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_2771_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8VaNOYH-jOfT",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "!cp ./unet_membrane.hdf5 /content/drive/My\\ Drive/"
   ],
   "execution_count": 0,
   "outputs": []
  }
 ]
}