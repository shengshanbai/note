{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16NQPTsN0rqE",
        "colab_type": "text"
      },
      "source": [
        "初始化mxnet环境"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIm4lx_O0wVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update && sudo apt-get install -y build-essential git libgfortran3\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin\n",
        "!sudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
        "!wget http://developer.download.nvidia.com/compute/cuda/10.1/Prod/local_installers/cuda-repo-ubuntu1804-10-1-local-10.1.243-418.87.00_1.0-1_amd64.deb\n",
        "!sudo dpkg -i cuda-repo-ubuntu1804-10-1-local-10.1.243-418.87.00_1.0-1_amd64.deb\n",
        "!sudo apt-key add /var/cuda-repo-10-1-local-10.1.243-418.87.00/7fa2af80.pub\n",
        "!sudo apt-get update\n",
        "!sudo apt-get -y install cuda\n",
        "!cat /usr/local/cuda/version.txt\n",
        "!pip install mxnet-cu100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZE6Fdc_0yU0",
        "colab_type": "text"
      },
      "source": [
        "原样导入d2lzh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3-RiX4T1V3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import tarfile\n",
        "import time\n",
        "import zipfile\n",
        "\n",
        "from IPython import display\n",
        "from matplotlib import pyplot as plt\n",
        "import mxnet as mx\n",
        "from mxnet import autograd, gluon, image, init, nd\n",
        "from mxnet.contrib import text\n",
        "from mxnet.gluon import data as gdata, loss as gloss, nn, utils as gutils\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "VOC_CLASSES = ['background', 'aeroplane', 'bicycle', 'bird', 'boat',\n",
        "               'bottle', 'bus', 'car', 'cat', 'chair', 'cow',\n",
        "               'diningtable', 'dog', 'horse', 'motorbike', 'person',\n",
        "               'potted plant', 'sheep', 'sofa', 'train', 'tv/monitor']\n",
        "\n",
        "\n",
        "VOC_COLORMAP = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],\n",
        "                [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],\n",
        "                [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],\n",
        "                [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],\n",
        "                [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],\n",
        "                [0, 64, 128]]\n",
        "\n",
        "\n",
        "def bbox_to_rect(bbox, color):\n",
        "    \"\"\"Convert bounding box to matplotlib format.\"\"\"\n",
        "    return plt.Rectangle(xy=(bbox[0], bbox[1]), width=bbox[2]-bbox[0],\n",
        "                         height=bbox[3]-bbox[1], fill=False, edgecolor=color,\n",
        "                         linewidth=2)\n",
        "\n",
        "\n",
        "class Benchmark():\n",
        "    \"\"\"Benchmark programs.\"\"\"\n",
        "    def __init__(self, prefix=None):\n",
        "        self.prefix = prefix + ' ' if prefix else ''\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.start = time.time()\n",
        "\n",
        "    def __exit__(self, *args):\n",
        "        print('%stime: %.4f sec' % (self.prefix, time.time() - self.start))\n",
        "\n",
        "\n",
        "def corr2d(X, K):\n",
        "    \"\"\"Compute 2D cross-correlation.\"\"\"\n",
        "    h, w = K.shape\n",
        "    Y = nd.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
        "    for i in range(Y.shape[0]):\n",
        "        for j in range(Y.shape[1]):\n",
        "            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()\n",
        "    return Y\n",
        "\n",
        "\n",
        "def count_tokens(samples):\n",
        "    \"\"\"Count tokens in the data set.\"\"\"\n",
        "    token_counter = collections.Counter()\n",
        "    for sample in samples:\n",
        "        for token in sample:\n",
        "            if token not in token_counter:\n",
        "                token_counter[token] = 1\n",
        "            else:\n",
        "                token_counter[token] += 1\n",
        "    return token_counter\n",
        "\n",
        "\n",
        "def data_iter(batch_size, features, labels):\n",
        "    \"\"\"Iterate through a data set.\"\"\"\n",
        "    num_examples = len(features)\n",
        "    indices = list(range(num_examples))\n",
        "    random.shuffle(indices)\n",
        "    for i in range(0, num_examples, batch_size):\n",
        "        j = nd.array(indices[i: min(i + batch_size, num_examples)])\n",
        "        yield features.take(j), labels.take(j)\n",
        "\n",
        "\n",
        "def data_iter_consecutive(corpus_indices, batch_size, num_steps, ctx=None):\n",
        "    \"\"\"Sample mini-batches in a consecutive order from sequential data.\"\"\"\n",
        "    corpus_indices = nd.array(corpus_indices, ctx=ctx)\n",
        "    data_len = len(corpus_indices)\n",
        "    batch_len = data_len // batch_size\n",
        "    indices = corpus_indices[0 : batch_size * batch_len].reshape((\n",
        "        batch_size, batch_len))\n",
        "    epoch_size = (batch_len - 1) // num_steps\n",
        "    for i in range(epoch_size):\n",
        "        i = i * num_steps\n",
        "        X = indices[:, i : i + num_steps]\n",
        "        Y = indices[:, i + 1 : i + num_steps + 1]\n",
        "        yield X, Y\n",
        "\n",
        "\n",
        "def data_iter_random(corpus_indices, batch_size, num_steps, ctx=None):\n",
        "    \"\"\"Sample mini-batches in a random order from sequential data.\"\"\"\n",
        "    num_examples = (len(corpus_indices) - 1) // num_steps\n",
        "    epoch_size = num_examples // batch_size\n",
        "    example_indices = list(range(num_examples))\n",
        "    random.shuffle(example_indices)\n",
        "\n",
        "    def _data(pos):\n",
        "        return corpus_indices[pos : pos + num_steps]\n",
        "\n",
        "    for i in range(epoch_size):\n",
        "        i = i * batch_size\n",
        "        batch_indices = example_indices[i : i + batch_size]\n",
        "        X = nd.array(\n",
        "            [_data(j * num_steps) for j in batch_indices], ctx=ctx)\n",
        "        Y = nd.array(\n",
        "            [_data(j * num_steps + 1) for j in batch_indices], ctx=ctx)\n",
        "        yield X, Y\n",
        "\n",
        "\n",
        "def download_imdb(data_dir='../data'):\n",
        "    \"\"\"Download the IMDB data set for sentiment analysis.\"\"\"\n",
        "    url = ('http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz')\n",
        "    sha1 = '01ada507287d82875905620988597833ad4e0903'\n",
        "    fname = gutils.download(url, data_dir, sha1_hash=sha1)\n",
        "    with tarfile.open(fname, 'r') as f:\n",
        "        f.extractall(data_dir)\n",
        "\n",
        "\n",
        "def _download_pikachu(data_dir):\n",
        "    root_url = ('https://apache-mxnet.s3-accelerate.amazonaws.com/'\n",
        "                'gluon/dataset/pikachu/')\n",
        "    dataset = {'train.rec': 'e6bcb6ffba1ac04ff8a9b1115e650af56ee969c8',\n",
        "               'train.idx': 'dcf7318b2602c06428b9988470c731621716c393',\n",
        "               'val.rec': 'd6c33f799b4d058e82f2cb5bd9a976f69d72d520'}\n",
        "    for k, v in dataset.items():\n",
        "        gutils.download(root_url + k, os.path.join(data_dir, k), sha1_hash=v)\n",
        "\n",
        "\n",
        "def download_voc_pascal(data_dir='../data'):\n",
        "    \"\"\"Download the Pascal VOC2012 Dataset.\"\"\"\n",
        "    voc_dir = os.path.join(data_dir, 'VOCdevkit/VOC2012')\n",
        "    url = ('http://host.robots.ox.ac.uk/pascal/VOC/voc2012'\n",
        "           '/VOCtrainval_11-May-2012.tar')\n",
        "    sha1 = '4e443f8a2eca6b1dac8a6c57641b67dd40621a49'\n",
        "    fname = gutils.download(url, data_dir, sha1_hash=sha1)\n",
        "    with tarfile.open(fname, 'r') as f:\n",
        "        f.extractall(data_dir)\n",
        "    return voc_dir\n",
        "\n",
        "\n",
        "def evaluate_accuracy(data_iter, net, ctx=[mx.cpu()]):\n",
        "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
        "    if isinstance(ctx, mx.Context):\n",
        "        ctx = [ctx]\n",
        "    acc_sum, n = nd.array([0]), 0\n",
        "    for batch in data_iter:\n",
        "        features, labels, _ = _get_batch(batch, ctx)\n",
        "        for X, y in zip(features, labels):\n",
        "            y = y.astype('float32')\n",
        "            acc_sum += (net(X).argmax(axis=1) == y).sum().copyto(mx.cpu())\n",
        "            n += y.size\n",
        "        acc_sum.wait_to_read()\n",
        "    return acc_sum.asscalar() / n\n",
        "\n",
        "\n",
        "def _get_batch(batch, ctx):\n",
        "    \"\"\"Return features and labels on ctx.\"\"\"\n",
        "    features, labels = batch\n",
        "    if labels.dtype != features.dtype:\n",
        "        labels = labels.astype(features.dtype)\n",
        "    return (gutils.split_and_load(features, ctx),\n",
        "            gutils.split_and_load(labels, ctx), features.shape[0])\n",
        "\n",
        "\n",
        "def get_data_ch7():\n",
        "    \"\"\"Get the data set used in Chapter 7.\"\"\"\n",
        "    data = np.genfromtxt('../data/airfoil_self_noise.dat', delimiter='\\t')\n",
        "    data = (data - data.mean(axis=0)) / data.std(axis=0)\n",
        "    return nd.array(data[:, :-1]), nd.array(data[:, -1])\n",
        "\n",
        "\n",
        "def get_fashion_mnist_labels(labels):\n",
        "    \"\"\"Get text label for fashion mnist.\"\"\"\n",
        "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
        "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
        "    return [text_labels[int(i)] for i in labels]\n",
        "\n",
        "\n",
        "def get_tokenized_imdb(data):\n",
        "    \"\"\"Get the tokenized IMDB data set for sentiment analysis.\"\"\"\n",
        "    def tokenizer(text):\n",
        "        return [tok.lower() for tok in text.split(' ')]\n",
        "    return [tokenizer(review) for review, _ in data]\n",
        "\n",
        "\n",
        "def get_vocab_imdb(data):\n",
        "    \"\"\"Get the vocab for the IMDB data set for sentiment analysis.\"\"\"\n",
        "    tokenized_data = get_tokenized_imdb(data)\n",
        "    counter = collections.Counter([tk for st in tokenized_data for tk in st])\n",
        "    return text.vocab.Vocabulary(counter, min_freq=5)\n",
        "\n",
        "\n",
        "def grad_clipping(params, theta, ctx):\n",
        "    \"\"\"Clip the gradient.\"\"\"\n",
        "    if theta is not None:\n",
        "        norm = nd.array([0], ctx)\n",
        "        for param in params:\n",
        "            norm += (param.grad ** 2).sum()\n",
        "        norm = norm.sqrt().asscalar()\n",
        "        if norm > theta:\n",
        "            for param in params:\n",
        "                param.grad[:] *= theta / norm\n",
        "\n",
        "\n",
        "def linreg(X, w, b):\n",
        "    \"\"\"Linear regression.\"\"\"\n",
        "    return nd.dot(X, w) + b\n",
        "\n",
        "\n",
        "def load_data_fashion_mnist(batch_size, resize=None, root=os.path.join(\n",
        "        '~', '.mxnet', 'datasets', 'fashion-mnist')):\n",
        "    \"\"\"Download the fashion mnist dataset and then load into memory.\"\"\"\n",
        "    root = os.path.expanduser(root)\n",
        "    transformer = []\n",
        "    if resize:\n",
        "        transformer += [gdata.vision.transforms.Resize(resize)]\n",
        "    transformer += [gdata.vision.transforms.ToTensor()]\n",
        "    transformer = gdata.vision.transforms.Compose(transformer)\n",
        "\n",
        "    mnist_train = gdata.vision.FashionMNIST(root=root, train=True)\n",
        "    mnist_test = gdata.vision.FashionMNIST(root=root, train=False)\n",
        "    num_workers = 0 if sys.platform.startswith('win32') else 4\n",
        "\n",
        "    train_iter = gdata.DataLoader(mnist_train.transform_first(transformer),\n",
        "                                  batch_size, shuffle=True,\n",
        "                                  num_workers=num_workers)\n",
        "    test_iter = gdata.DataLoader(mnist_test.transform_first(transformer),\n",
        "                                 batch_size, shuffle=False,\n",
        "                                 num_workers=num_workers)\n",
        "    return train_iter, test_iter\n",
        "\n",
        "\n",
        "def load_data_jay_lyrics():\n",
        "    \"\"\"Load the Jay Chou lyric data set (available in the Chinese book).\"\"\"\n",
        "    with zipfile.ZipFile('../data/jaychou_lyrics.txt.zip') as zin:\n",
        "        with zin.open('jaychou_lyrics.txt') as f:\n",
        "            corpus_chars = f.read().decode('utf-8')\n",
        "    corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n",
        "    corpus_chars = corpus_chars[0:10000]\n",
        "    idx_to_char = list(set(corpus_chars))\n",
        "    char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
        "    vocab_size = len(char_to_idx)\n",
        "    corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
        "    return corpus_indices, char_to_idx, idx_to_char, vocab_size\n",
        "\n",
        "\n",
        "def load_data_pikachu(batch_size, edge_size=256):\n",
        "    \"\"\"Download the pikachu dataest and then load into memory.\"\"\"\n",
        "    data_dir = '../data/pikachu'\n",
        "    _download_pikachu(data_dir)\n",
        "    train_iter = image.ImageDetIter(\n",
        "        path_imgrec=os.path.join(data_dir, 'train.rec'),\n",
        "        path_imgidx=os.path.join(data_dir, 'train.idx'),\n",
        "        batch_size=batch_size,\n",
        "        data_shape=(3, edge_size, edge_size),\n",
        "        shuffle=True,\n",
        "        rand_crop=1,\n",
        "        min_object_covered=0.95,\n",
        "        max_attempts=200)\n",
        "    val_iter = image.ImageDetIter(\n",
        "        path_imgrec=os.path.join(data_dir, 'val.rec'),\n",
        "        batch_size=batch_size,\n",
        "        data_shape=(3, edge_size, edge_size),\n",
        "        shuffle=False)\n",
        "    return train_iter, val_iter\n",
        "\n",
        "\n",
        "def load_data_time_machine():\n",
        "    \"\"\"Load the time machine data set (available in the English book).\"\"\"\n",
        "    with open('../data/timemachine.txt') as f:\n",
        "        corpus_chars = f.read()\n",
        "    corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ').lower()\n",
        "    corpus_chars = corpus_chars[0:10000]\n",
        "    idx_to_char = list(set(corpus_chars))\n",
        "    char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n",
        "    vocab_size = len(char_to_idx)\n",
        "    corpus_indices = [char_to_idx[char] for char in corpus_chars]\n",
        "    return corpus_indices, char_to_idx, idx_to_char, vocab_size\n",
        "\n",
        "\n",
        "def _make_list(obj, default_values=None):\n",
        "    if obj is None:\n",
        "        obj = default_values\n",
        "    elif not isinstance(obj, (list, tuple)):\n",
        "        obj = [obj]\n",
        "    return obj\n",
        "\n",
        "\n",
        "def mkdir_if_not_exist(path):\n",
        "    \"\"\"Make a directory if it does not exist.\"\"\"\n",
        "    if not os.path.exists(os.path.join(*path)):\n",
        "        os.makedirs(os.path.join(*path))\n",
        "\n",
        "\n",
        "def predict_rnn(prefix, num_chars, rnn, params, init_rnn_state,\n",
        "                num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx):\n",
        "    \"\"\"Predict next chars with a RNN model\"\"\"\n",
        "    state = init_rnn_state(1, num_hiddens, ctx)\n",
        "    output = [char_to_idx[prefix[0]]]\n",
        "    for t in range(num_chars + len(prefix) - 1):\n",
        "        X = to_onehot(nd.array([output[-1]], ctx=ctx), vocab_size)\n",
        "        (Y, state) = rnn(X, state, params)\n",
        "        if t < len(prefix) - 1:\n",
        "            output.append(char_to_idx[prefix[t + 1]])\n",
        "        else:\n",
        "            output.append(int(Y[0].argmax(axis=1).asscalar()))\n",
        "    return ''.join([idx_to_char[i] for i in output])\n",
        "\n",
        "\n",
        "def predict_rnn_gluon(prefix, num_chars, model, vocab_size, ctx, idx_to_char,\n",
        "                      char_to_idx):\n",
        "    \"\"\"Precit next chars with a Gluon RNN model\"\"\"\n",
        "    state = model.begin_state(batch_size=1, ctx=ctx)\n",
        "    output = [char_to_idx[prefix[0]]]\n",
        "    for t in range(num_chars + len(prefix) - 1):\n",
        "        X = nd.array([output[-1]], ctx=ctx).reshape((1, 1))\n",
        "        (Y, state) = model(X, state)\n",
        "        if t < len(prefix) - 1:\n",
        "            output.append(char_to_idx[prefix[t + 1]])\n",
        "        else:\n",
        "            output.append(int(Y.argmax(axis=1).asscalar()))\n",
        "    return ''.join([idx_to_char[i] for i in output])\n",
        "\n",
        "\n",
        "def predict_sentiment(net, vocab, sentence):\n",
        "    \"\"\"Predict the sentiment of a given sentence.\"\"\"\n",
        "    sentence = nd.array(vocab.to_indices(sentence), ctx=try_gpu())\n",
        "    label = nd.argmax(net(sentence.reshape((1, -1))), axis=1)\n",
        "    return 'positive' if label.asscalar() == 1 else 'negative'\n",
        "\n",
        "\n",
        "def preprocess_imdb(data, vocab):\n",
        "    \"\"\"Preprocess the IMDB data set for sentiment analysis.\"\"\"\n",
        "    max_l = 500\n",
        "\n",
        "    def pad(x):\n",
        "        return x[:max_l] if len(x) > max_l else x + [0] * (max_l - len(x))\n",
        "\n",
        "    tokenized_data = get_tokenized_imdb(data)\n",
        "    features = nd.array([pad(vocab.to_indices(x)) for x in tokenized_data])\n",
        "    labels = nd.array([score for _, score in data])\n",
        "    return features, labels\n",
        "\n",
        "\n",
        "def read_imdb(folder='train'):\n",
        "    \"\"\"Read the IMDB data set for sentiment analysis.\"\"\"\n",
        "    data = []\n",
        "    for label in ['pos', 'neg']:\n",
        "        folder_name = os.path.join('../data/aclImdb/', folder, label)\n",
        "        for file in os.listdir(folder_name):\n",
        "            with open(os.path.join(folder_name, file), 'rb') as f:\n",
        "                review = f.read().decode('utf-8').replace('\\n', '').lower()\n",
        "                data.append([review, 1 if label == 'pos' else 0])\n",
        "    random.shuffle(data)\n",
        "    return data\n",
        "\n",
        "\n",
        "def read_voc_images(root='../data/VOCdevkit/VOC2012', is_train=True):\n",
        "    \"\"\"Read VOC images.\"\"\"\n",
        "    txt_fname = '%s/ImageSets/Segmentation/%s' % (\n",
        "        root, 'train.txt' if is_train else 'val.txt')\n",
        "    with open(txt_fname, 'r') as f:\n",
        "        images = f.read().split()\n",
        "    features, labels = [None] * len(images), [None] * len(images)\n",
        "    for i, fname in enumerate(images):\n",
        "        features[i] = image.imread('%s/JPEGImages/%s.jpg' % (root, fname))\n",
        "        labels[i] = image.imread(\n",
        "            '%s/SegmentationClass/%s.png' % (root, fname))\n",
        "    return features, labels\n",
        "\n",
        "\n",
        "class Residual(nn.Block):\n",
        "    \"\"\"The residual block.\"\"\"\n",
        "    def __init__(self, num_channels, use_1x1conv=False, strides=1, **kwargs):\n",
        "        super(Residual, self).__init__(**kwargs)\n",
        "        self.conv1 = nn.Conv2D(num_channels, kernel_size=3, padding=1,\n",
        "                               strides=strides)\n",
        "        self.conv2 = nn.Conv2D(num_channels, kernel_size=3, padding=1)\n",
        "        if use_1x1conv:\n",
        "            self.conv3 = nn.Conv2D(num_channels, kernel_size=1,\n",
        "                                   strides=strides)\n",
        "        else:\n",
        "            self.conv3 = None\n",
        "        self.bn1 = nn.BatchNorm()\n",
        "        self.bn2 = nn.BatchNorm()\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = nd.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.bn2(self.conv2(Y))\n",
        "        if self.conv3:\n",
        "            X = self.conv3(X)\n",
        "        return nd.relu(Y + X)\n",
        "\n",
        "\n",
        "def resnet18(num_classes):\n",
        "    \"\"\"The ResNet-18 model.\"\"\"\n",
        "    net = nn.Sequential()\n",
        "    net.add(nn.Conv2D(64, kernel_size=3, strides=1, padding=1),\n",
        "            nn.BatchNorm(), nn.Activation('relu'))\n",
        "\n",
        "    def resnet_block(num_channels, num_residuals, first_block=False):\n",
        "        blk = nn.Sequential()\n",
        "        for i in range(num_residuals):\n",
        "            if i == 0 and not first_block:\n",
        "                blk.add(Residual(num_channels, use_1x1conv=True, strides=2))\n",
        "            else:\n",
        "                blk.add(Residual(num_channels))\n",
        "        return blk\n",
        "\n",
        "    net.add(resnet_block(64, 2, first_block=True),\n",
        "            resnet_block(128, 2),\n",
        "            resnet_block(256, 2),\n",
        "            resnet_block(512, 2))\n",
        "    net.add(nn.GlobalAvgPool2D(), nn.Dense(num_classes))\n",
        "    return net\n",
        "\n",
        "\n",
        "class RNNModel(nn.Block):\n",
        "    \"\"\"RNN model.\"\"\"\n",
        "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
        "        super(RNNModel, self).__init__(**kwargs)\n",
        "        self.rnn = rnn_layer\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dense = nn.Dense(vocab_size)\n",
        "\n",
        "    def forward(self, inputs, state):\n",
        "        X = nd.one_hot(inputs.T, self.vocab_size)\n",
        "        Y, state = self.rnn(X, state)\n",
        "        output = self.dense(Y.reshape((-1, Y.shape[-1])))\n",
        "        return output, state\n",
        "\n",
        "    def begin_state(self, *args, **kwargs):\n",
        "        return self.rnn.begin_state(*args, **kwargs)\n",
        "\n",
        "\n",
        "def semilogy(x_vals, y_vals, x_label, y_label, x2_vals=None, y2_vals=None,\n",
        "             legend=None, figsize=(3.5, 2.5)):\n",
        "    \"\"\"Plot x and log(y).\"\"\"\n",
        "    set_figsize(figsize)\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "    plt.semilogy(x_vals, y_vals)\n",
        "    if x2_vals and y2_vals:\n",
        "        plt.semilogy(x2_vals, y2_vals, linestyle=':')\n",
        "        plt.legend(legend)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def set_figsize(figsize=(3.5, 2.5)):\n",
        "    \"\"\"Set matplotlib figure size.\"\"\"\n",
        "    use_svg_display()\n",
        "    plt.rcParams['figure.figsize'] = figsize\n",
        "\n",
        "\n",
        "def sgd(params, lr, batch_size):\n",
        "    \"\"\"Mini-batch stochastic gradient descent.\"\"\"\n",
        "    for param in params:\n",
        "        param[:] = param - lr * param.grad / batch_size\n",
        "\n",
        "\n",
        "def show_bboxes(axes, bboxes, labels=None, colors=None):\n",
        "    \"\"\"Show bounding boxes.\"\"\"\n",
        "    labels = _make_list(labels)\n",
        "    colors = _make_list(colors, ['b', 'g', 'r', 'm', 'k'])\n",
        "    for i, bbox in enumerate(bboxes):\n",
        "        color = colors[i % len(colors)]\n",
        "        rect = bbox_to_rect(bbox.asnumpy(), color)\n",
        "        axes.add_patch(rect)\n",
        "        if labels and len(labels) > i:\n",
        "            text_color = 'k' if color == 'w' else 'w'\n",
        "            axes.text(rect.xy[0], rect.xy[1], labels[i],\n",
        "                      va='center', ha='center', fontsize=9, color=text_color,\n",
        "                      bbox=dict(facecolor=color, lw=0))\n",
        "\n",
        "\n",
        "def show_fashion_mnist(images, labels):\n",
        "    \"\"\"Plot Fashion-MNIST images with labels.\"\"\"\n",
        "    use_svg_display()\n",
        "    _, figs = plt.subplots(1, len(images), figsize=(12, 12))\n",
        "    for f, img, lbl in zip(figs, images, labels):\n",
        "        f.imshow(img.reshape((28, 28)).asnumpy())\n",
        "        f.set_title(lbl)\n",
        "        f.axes.get_xaxis().set_visible(False)\n",
        "        f.axes.get_yaxis().set_visible(False)\n",
        "\n",
        "\n",
        "def show_images(imgs, num_rows, num_cols, scale=2):\n",
        "    \"\"\"Plot a list of images.\"\"\"\n",
        "    figsize = (num_cols * scale, num_rows * scale)\n",
        "    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
        "    for i in range(num_rows):\n",
        "        for j in range(num_cols):\n",
        "            axes[i][j].imshow(imgs[i * num_cols + j].asnumpy())\n",
        "            axes[i][j].axes.get_xaxis().set_visible(False)\n",
        "            axes[i][j].axes.get_yaxis().set_visible(False)\n",
        "    return axes\n",
        "\n",
        "\n",
        "def show_trace_2d(f, res):\n",
        "    \"\"\"Show the trace of 2d variables during optimization.\"\"\"\n",
        "    x1, x2 = zip(*res)\n",
        "    set_figsize()\n",
        "    plt.plot(x1, x2, '-o', color='#ff7f0e')\n",
        "    x1 = np.arange(-5.5, 1.0, 0.1)\n",
        "    x2 = np.arange(min(-3.0, min(x2) - 1), max(1.0, max(x2) + 1), 0.1)\n",
        "    x1, x2 = np.meshgrid(x1, x2)\n",
        "    plt.contour(x1, x2, f(x1, x2), colors='#1f77b4')\n",
        "    plt.xlabel('x1')\n",
        "    plt.ylabel('x2')\n",
        "\n",
        "\n",
        "def squared_loss(y_hat, y):\n",
        "    \"\"\"Squared loss.\"\"\"\n",
        "    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2\n",
        "\n",
        "\n",
        "def to_onehot(X, size):\n",
        "    \"\"\"Represent inputs with one-hot encoding.\"\"\"\n",
        "    return [nd.one_hot(x, size) for x in X.T]\n",
        "\n",
        "\n",
        "def train(train_iter, test_iter, net, loss, trainer, ctx, num_epochs):\n",
        "    \"\"\"Train and evaluate a model.\"\"\"\n",
        "    print('training on', ctx)\n",
        "    if isinstance(ctx, mx.Context):\n",
        "        ctx = [ctx]\n",
        "    for epoch in range(num_epochs):\n",
        "        train_l_sum, train_acc_sum, n, m, start = 0.0, 0.0, 0, 0, time.time()\n",
        "        for i, batch in enumerate(train_iter):\n",
        "            Xs, ys, batch_size = _get_batch(batch, ctx)\n",
        "            ls = []\n",
        "            with autograd.record():\n",
        "                y_hats = [net(X) for X in Xs]\n",
        "                ls = [loss(y_hat, y) for y_hat, y in zip(y_hats, ys)]\n",
        "            for l in ls:\n",
        "                l.backward()\n",
        "            trainer.step(batch_size)\n",
        "            train_l_sum += sum([l.sum().asscalar() for l in ls])\n",
        "            n += sum([l.size for l in ls])\n",
        "            train_acc_sum += sum([(y_hat.argmax(axis=1) == y).sum().asscalar()\n",
        "                                 for y_hat, y in zip(y_hats, ys)])\n",
        "            m += sum([y.size for y in ys])\n",
        "        test_acc = evaluate_accuracy(test_iter, net, ctx)\n",
        "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '\n",
        "              'time %.1f sec'\n",
        "              % (epoch + 1, train_l_sum / n, train_acc_sum / m, test_acc,\n",
        "                 time.time() - start))\n",
        "\n",
        "\n",
        "def train_2d(trainer):\n",
        "    \"\"\"Optimize the objective function of 2d variables with a customized trainer.\"\"\"\n",
        "    x1, x2 = -5, -2\n",
        "    s_x1, s_x2 = 0, 0\n",
        "    res = [(x1, x2)]\n",
        "    for i in range(20):\n",
        "        x1, x2, s_x1, s_x2 = trainer(x1, x2, s_x1, s_x2)\n",
        "        res.append((x1, x2))\n",
        "    print('epoch %d, x1 %f, x2 %f' % (i+1, x1, x2))\n",
        "    return res\n",
        "\n",
        "\n",
        "def train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,\n",
        "                          vocab_size, ctx, corpus_indices, idx_to_char,\n",
        "                          char_to_idx, is_random_iter, num_epochs, num_steps,\n",
        "                          lr, clipping_theta, batch_size, pred_period,\n",
        "                          pred_len, prefixes):\n",
        "    \"\"\"Train an RNN model and predict the next item in the sequence.\"\"\"\n",
        "    if is_random_iter:\n",
        "        data_iter_fn = data_iter_random\n",
        "    else:\n",
        "        data_iter_fn = data_iter_consecutive\n",
        "    params = get_params()\n",
        "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        if not is_random_iter:\n",
        "            state = init_rnn_state(batch_size, num_hiddens, ctx)\n",
        "        l_sum, n, start = 0.0, 0, time.time()\n",
        "        data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, ctx)\n",
        "        for X, Y in data_iter:\n",
        "            if is_random_iter:\n",
        "                state = init_rnn_state(batch_size, num_hiddens, ctx)\n",
        "            else:\n",
        "                for s in state:\n",
        "                    s.detach()\n",
        "            with autograd.record():\n",
        "                inputs = to_onehot(X, vocab_size)\n",
        "                (outputs, state) = rnn(inputs, state, params)\n",
        "                outputs = nd.concat(*outputs, dim=0)\n",
        "                y = Y.T.reshape((-1,))\n",
        "                l = loss(outputs, y).mean()\n",
        "            l.backward()\n",
        "            grad_clipping(params, clipping_theta, ctx)\n",
        "            sgd(params, lr, 1)\n",
        "            l_sum += l.asscalar() * y.size\n",
        "            n += y.size\n",
        "\n",
        "        if (epoch + 1) % pred_period == 0:\n",
        "            print('epoch %d, perplexity %f, time %.2f sec' % (\n",
        "                epoch + 1, math.exp(l_sum / n), time.time() - start))\n",
        "            for prefix in prefixes:\n",
        "                print(' -', predict_rnn(\n",
        "                    prefix, pred_len, rnn, params, init_rnn_state,\n",
        "                    num_hiddens, vocab_size, ctx, idx_to_char, char_to_idx))\n",
        "\n",
        "\n",
        "def train_and_predict_rnn_gluon(model, num_hiddens, vocab_size, ctx,\n",
        "                                corpus_indices, idx_to_char, char_to_idx,\n",
        "                                num_epochs, num_steps, lr, clipping_theta,\n",
        "                                batch_size, pred_period, pred_len, prefixes):\n",
        "    \"\"\"Train an Gluon RNN model and predict the next item in the sequence.\"\"\"\n",
        "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
        "    model.initialize(ctx=ctx, force_reinit=True, init=init.Normal(0.01))\n",
        "    trainer = gluon.Trainer(model.collect_params(), 'sgd',\n",
        "                            {'learning_rate': lr, 'momentum': 0, 'wd': 0})\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        l_sum, n, start = 0.0, 0, time.time()\n",
        "        data_iter = data_iter_consecutive(\n",
        "            corpus_indices, batch_size, num_steps, ctx)\n",
        "        state = model.begin_state(batch_size=batch_size, ctx=ctx)\n",
        "        for X, Y in data_iter:\n",
        "            for s in state:\n",
        "                s.detach()\n",
        "            with autograd.record():\n",
        "                (output, state) = model(X, state)\n",
        "                y = Y.T.reshape((-1,))\n",
        "                l = loss(output, y).mean()\n",
        "            l.backward()\n",
        "            params = [p.data() for p in model.collect_params().values()]\n",
        "            grad_clipping(params, clipping_theta, ctx)\n",
        "            trainer.step(1)\n",
        "            l_sum += l.asscalar() * y.size\n",
        "            n += y.size\n",
        "\n",
        "        if (epoch + 1) % pred_period == 0:\n",
        "            print('epoch %d, perplexity %f, time %.2f sec' % (\n",
        "                epoch + 1, math.exp(l_sum / n), time.time() - start))\n",
        "            for prefix in prefixes:\n",
        "                print(' -', predict_rnn_gluon(\n",
        "                    prefix, pred_len, model, vocab_size, ctx, idx_to_char,\n",
        "                    char_to_idx))\n",
        "\n",
        "\n",
        "def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\n",
        "              params=None, lr=None, trainer=None):\n",
        "    \"\"\"Train and evaluate a model with CPU.\"\"\"\n",
        "    for epoch in range(num_epochs):\n",
        "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
        "        for X, y in train_iter:\n",
        "            with autograd.record():\n",
        "                y_hat = net(X)\n",
        "                l = loss(y_hat, y).sum()\n",
        "            l.backward()\n",
        "            if trainer is None:\n",
        "                sgd(params, lr, batch_size)\n",
        "            else:\n",
        "                trainer.step(batch_size)\n",
        "            y = y.astype('float32')\n",
        "            train_l_sum += l.asscalar()\n",
        "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
        "            n += y.size\n",
        "        test_acc = evaluate_accuracy(test_iter, net)\n",
        "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
        "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
        "\n",
        "\n",
        "def train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx,\n",
        "              num_epochs):\n",
        "    \"\"\"Train and evaluate a model with CPU or GPU.\"\"\"\n",
        "    print('training on', ctx)\n",
        "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
        "    for epoch in range(num_epochs):\n",
        "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
        "        for X, y in train_iter:\n",
        "            X, y = X.as_in_context(ctx), y.as_in_context(ctx)\n",
        "            with autograd.record():\n",
        "                y_hat = net(X)\n",
        "                l = loss(y_hat, y).sum()\n",
        "            l.backward()\n",
        "            trainer.step(batch_size)\n",
        "            y = y.astype('float32')\n",
        "            train_l_sum += l.asscalar()\n",
        "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
        "            n += y.size\n",
        "        test_acc = evaluate_accuracy(test_iter, net, ctx)\n",
        "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '\n",
        "              'time %.1f sec'\n",
        "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc,\n",
        "                 time.time() - start))\n",
        "\n",
        "\n",
        "def train_ch7(trainer_fn, states, hyperparams, features, labels, batch_size=10,\n",
        "              num_epochs=2):\n",
        "    \"\"\"Train a linear regression model.\"\"\"\n",
        "    net, loss = linreg, squared_loss\n",
        "    w, b = nd.random.normal(scale=0.01, shape=(features.shape[1], 1)), nd.zeros(1)\n",
        "    w.attach_grad()\n",
        "    b.attach_grad()\n",
        "\n",
        "    def eval_loss():\n",
        "        return loss(net(features, w, b), labels).mean().asscalar()\n",
        "\n",
        "    ls = [eval_loss()]\n",
        "    data_iter = gdata.DataLoader(\n",
        "        gdata.ArrayDataset(features, labels), batch_size, shuffle=True)\n",
        "    for _ in range(num_epochs):\n",
        "        start = time.time()\n",
        "        for batch_i, (X, y) in enumerate(data_iter):\n",
        "            with autograd.record():\n",
        "                l = loss(net(X, w, b), y).mean()\n",
        "            l.backward()\n",
        "            trainer_fn([w, b], states, hyperparams)\n",
        "            if (batch_i + 1) * batch_size % 100 == 0:\n",
        "                ls.append(eval_loss())\n",
        "    print('loss: %f, %f sec per epoch' % (ls[-1], time.time() - start))\n",
        "    set_figsize()\n",
        "    plt.plot(np.linspace(0, num_epochs, len(ls)), ls)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "\n",
        "\n",
        "def train_gluon_ch7(trainer_name, trainer_hyperparams, features, labels,\n",
        "                    batch_size=10, num_epochs=2):\n",
        "    \"\"\"Train a linear regression model with a given Gluon trainer.\"\"\"\n",
        "    net = nn.Sequential()\n",
        "    net.add(nn.Dense(1))\n",
        "    net.initialize(init.Normal(sigma=0.01))\n",
        "    loss = gloss.L2Loss()\n",
        "\n",
        "    def eval_loss():\n",
        "        return loss(net(features), labels).mean().asscalar()\n",
        "\n",
        "    ls = [eval_loss()]\n",
        "    data_iter = gdata.DataLoader(\n",
        "        gdata.ArrayDataset(features, labels), batch_size, shuffle=True)\n",
        "    trainer = gluon.Trainer(net.collect_params(),\n",
        "                            trainer_name, trainer_hyperparams)\n",
        "    for _ in range(num_epochs):\n",
        "        start = time.time()\n",
        "        for batch_i, (X, y) in enumerate(data_iter):\n",
        "            with autograd.record():\n",
        "                l = loss(net(X), y)\n",
        "            l.backward()\n",
        "            trainer.step(batch_size)\n",
        "            if (batch_i + 1) * batch_size % 100 == 0:\n",
        "                ls.append(eval_loss())\n",
        "    print('loss: %f, %f sec per epoch' % (ls[-1], time.time() - start))\n",
        "    set_figsize()\n",
        "    plt.plot(np.linspace(0, num_epochs, len(ls)), ls)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "\n",
        "\n",
        "def try_all_gpus():\n",
        "    \"\"\"Return all available GPUs, or [mx.cpu()] if there is no GPU.\"\"\"\n",
        "    ctxes = []\n",
        "    try:\n",
        "        for i in range(16):\n",
        "            ctx = mx.gpu(i)\n",
        "            _ = nd.array([0], ctx=ctx)\n",
        "            ctxes.append(ctx)\n",
        "    except mx.base.MXNetError:\n",
        "        pass\n",
        "    if not ctxes:\n",
        "        ctxes = [mx.cpu()]\n",
        "    return ctxes\n",
        "\n",
        "\n",
        "def try_gpu():\n",
        "    \"\"\"If GPU is available, return mx.gpu(0); else return mx.cpu().\"\"\"\n",
        "    try:\n",
        "        ctx = mx.gpu()\n",
        "        _ = nd.array([0], ctx=ctx)\n",
        "    except mx.base.MXNetError:\n",
        "        ctx = mx.cpu()\n",
        "    return ctx\n",
        "\n",
        "\n",
        "def use_svg_display():\n",
        "    \"\"\"Use svg format to display plot in jupyter\"\"\"\n",
        "    display.set_matplotlib_formats('svg')\n",
        "\n",
        "\n",
        "def voc_label_indices(colormap, colormap2label):\n",
        "    \"\"\"Assign label indices for Pascal VOC2012 Dataset.\"\"\"\n",
        "    colormap = colormap.astype('int32')\n",
        "    idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256\n",
        "           + colormap[:, :, 2])\n",
        "    return colormap2label[idx]\n",
        "\n",
        "\n",
        "def voc_rand_crop(feature, label, height, width):\n",
        "    \"\"\"Random cropping for images of the Pascal VOC2012 Dataset.\"\"\"\n",
        "    feature, rect = image.random_crop(feature, (width, height))\n",
        "    label = image.fixed_crop(label, *rect)\n",
        "    return feature, label\n",
        "\n",
        "\n",
        "class VOCSegDataset(gdata.Dataset):\n",
        "    \"\"\"The Pascal VOC2012 Dataset.\"\"\"\n",
        "    def __init__(self, is_train, crop_size, voc_dir, colormap2label):\n",
        "        self.rgb_mean = nd.array([0.485, 0.456, 0.406])\n",
        "        self.rgb_std = nd.array([0.229, 0.224, 0.225])\n",
        "        self.crop_size = crop_size\n",
        "        data, labels = read_voc_images(root=voc_dir, is_train=is_train)\n",
        "        self.data = [self.normalize_image(im) for im in self.filter(data)]\n",
        "        self.labels = self.filter(labels)\n",
        "        self.colormap2label = colormap2label\n",
        "        print('read ' + str(len(self.data)) + ' examples')\n",
        "\n",
        "    def normalize_image(self, data):\n",
        "        return (data.astype('float32') / 255 - self.rgb_mean) / self.rgb_std\n",
        "\n",
        "    def filter(self, images):\n",
        "        return [im for im in images if (\n",
        "            im.shape[0] >= self.crop_size[0] and\n",
        "            im.shape[1] >= self.crop_size[1])]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data, labels = voc_rand_crop(self.data[idx], self.labels[idx],\n",
        "                                     *self.crop_size)\n",
        "        return (data.transpose((2, 0, 1)),\n",
        "                voc_label_indices(labels, self.colormap2label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4zchj783QXR",
        "colab_type": "text"
      },
      "source": [
        "残差网络学习输出值和输入值之差，残差块实现如下："
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwiYLUqd3cuf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from mxnet import gluon, init, nd\n",
        "from mxnet.gluon import nn\n",
        "class Residual(nn.Block):  # 本类已保存在d2lzh包中方便以后使用\n",
        "  def __init__(self, num_channels, use_1x1conv=False, strides=1, **kwargs):\n",
        "    super(Residual, self).__init__(**kwargs)\n",
        "    self.conv1 = nn.Conv2D(num_channels, kernel_size=3, padding=1,\n",
        "                               strides=strides)\n",
        "    self.conv2 = nn.Conv2D(num_channels, kernel_size=3, padding=1)\n",
        "    if use_1x1conv:\n",
        "      self.conv3 = nn.Conv2D(num_channels, kernel_size=1,strides=strides)\n",
        "    else:\n",
        "      self.conv3 = None\n",
        "    self.bn1 = nn.BatchNorm()\n",
        "    self.bn2 = nn.BatchNorm()\n",
        "  def forward(self, X):\n",
        "    Y = nd.relu(self.bn1(self.conv1(X)))\n",
        "    Y = self.bn2(self.conv2(Y))\n",
        "    if self.conv3:\n",
        "      X = self.conv3(X)\n",
        "    return nd.relu(Y + X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEyee1z75HVj",
        "colab_type": "text"
      },
      "source": [
        "ResNet模型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SJKY5yz5IiT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = nn.Sequential()\n",
        "net.add(nn.Conv2D(64, kernel_size=7, strides=2, padding=3),\n",
        "        nn.BatchNorm(), nn.Activation('relu'),\n",
        "        nn.MaxPool2D(pool_size=3, strides=2, padding=1))\n",
        "\n",
        "def resnet_block(num_channels, num_residuals, first_block=False):\n",
        "  blk = nn.Sequential()\n",
        "  for i in range(num_residuals):\n",
        "    if i == 0 and not first_block:\n",
        "      blk.add(Residual(num_channels, use_1x1conv=True, strides=2))\n",
        "    else:\n",
        "      blk.add(Residual(num_channels))\n",
        "  return blk\n",
        "\n",
        "net.add(resnet_block(64, 2, first_block=True),\n",
        "        resnet_block(128, 2),\n",
        "        resnet_block(256, 2),\n",
        "        resnet_block(512, 2))\n",
        "net.add(nn.GlobalAvgPool2D(), nn.Dense(10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leaLAnFCvR7g",
        "colab_type": "text"
      },
      "source": [
        "下面我们在Fashion-MNIST数据集上训练ResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0yjavYrvTCu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "91db35cc-3979-4948-d05d-7852b63d5ba3"
      },
      "source": [
        "lr, num_epochs, batch_size, ctx = 0.05, 5, 256, try_gpu()\n",
        "net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier())\n",
        "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
        "train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=96)\n",
        "train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx,\n",
        "              num_epochs)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading /root/.mxnet/datasets/fashion-mnist/train-images-idx3-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/fashion-mnist/train-images-idx3-ubyte.gz...\n",
            "Downloading /root/.mxnet/datasets/fashion-mnist/train-labels-idx1-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/fashion-mnist/train-labels-idx1-ubyte.gz...\n",
            "Downloading /root/.mxnet/datasets/fashion-mnist/t10k-images-idx3-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/fashion-mnist/t10k-images-idx3-ubyte.gz...\n",
            "Downloading /root/.mxnet/datasets/fashion-mnist/t10k-labels-idx1-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/fashion-mnist/t10k-labels-idx1-ubyte.gz...\n",
            "training on gpu(0)\n",
            "epoch 1, loss 0.4873, train acc 0.830, test acc 0.879, time 89.6 sec\n",
            "epoch 2, loss 0.2528, train acc 0.907, test acc 0.844, time 83.5 sec\n",
            "epoch 3, loss 0.1902, train acc 0.930, test acc 0.906, time 83.5 sec\n",
            "epoch 4, loss 0.1447, train acc 0.948, test acc 0.904, time 83.5 sec\n",
            "epoch 5, loss 0.1057, train acc 0.963, test acc 0.916, time 83.6 sec\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}